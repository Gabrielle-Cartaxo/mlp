{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron - versão inicial\n",
    "\n",
    "Primeiramente fiz um perceptron com uma camada escondida e uma função sigmoide de ativação, ainda sem a utilização da função de custo e do treinamento para ver como o perceptron agia. Ao testar diferentes inputs, percebi que nenhuma predição chegava perto o suficiente para ser 1, sendo uma versão que não atende a porta XOR.\n",
    "\n",
    "Isso aconteceu porque por não possuir o treinamento, não houve ajuste dos pesos, o que faz com que as saídas não sejam ajustadas conforme o erro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0 1], Prediçãp: [0.82908799], Retorno: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, input_size, hidden_size, bias=1):\n",
    "        self.weights_input_hidden = np.random.rand(input_size, hidden_size)\n",
    "        self.weights_hidden_output = np.random.rand(hidden_size, 1)\n",
    "        self.bias_hidden = np.random.rand(hidden_size)\n",
    "        self.bias_output = np.random.rand(1)\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def forward_pass(self, data):\n",
    "        # Passagem pela camada escondida\n",
    "        self.hidden_input = np.dot(data, self.weights_input_hidden) + self.bias_hidden\n",
    "        self.hidden_output = self._sigmoid(self.hidden_input)\n",
    "        \n",
    "        # Passagem pela camada de saída\n",
    "        self.final_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output\n",
    "        self.final_output = self._sigmoid(self.final_input)\n",
    "        \n",
    "        return self.final_output\n",
    "\n",
    "    def predict(self, data):\n",
    "        output = self.forward_pass(data)\n",
    "        return output, 1 if output > 0.9 else 0\n",
    "\n",
    "# Exemplo de uso\n",
    "input_data = np.array([0, 1])\n",
    "perceptron = Perceptron(input_size=2, hidden_size=2)\n",
    "output, prediction = perceptron.predict(input_data)\n",
    "print(f\"Input: {input_data}, Prediçãp: {output}, Retorno: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adicionando Função de custo\n",
    "\n",
    "Aqui adicionei uma função de custo usando a média do erro quadrático, o que melhorou um pouco os números mais inda não foi suficiente para que as predições fossem corretas para uma porta XOR.\n",
    "\n",
    "Isso acontece porque, apesar do erro estar sendo calculado, esse valor ainda não está sendo utilizado para mudar os pesos, o que significa que os resultados dessa versão são muito parecidos com o da anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0 1], Predição: [0.85289369], Retorno: 0, Erro: 0.021640266170998238\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, input_size, hidden_size, bias=1):\n",
    "        self.weights_input_hidden = np.random.rand(input_size, hidden_size)\n",
    "        self.weights_hidden_output = np.random.rand(hidden_size, 1)\n",
    "        self.bias_hidden = np.random.rand(hidden_size)\n",
    "        self.bias_output = np.random.rand(1)\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def _mean_squared_error(self, predicted, actual):\n",
    "        return np.mean((predicted - actual) ** 2)\n",
    "\n",
    "    def forward_pass(self, data):\n",
    "        # Passagem pela camada escondida\n",
    "        self.hidden_input = np.dot(data, self.weights_input_hidden) + self.bias_hidden\n",
    "        self.hidden_output = self._sigmoid(self.hidden_input)\n",
    "        \n",
    "        # Passagem pela camada de saída\n",
    "        self.final_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output\n",
    "        self.final_output = self._sigmoid(self.final_input)\n",
    "        \n",
    "        return self.final_output\n",
    "\n",
    "    def predict(self, data):\n",
    "        output = self.forward_pass(data)\n",
    "        return output, 1 if output > 0.9 else 0\n",
    "\n",
    "# Exemplo de uso\n",
    "input_data = np.array([0, 1])\n",
    "actual_output = 1\n",
    "perceptron = Perceptron(input_size=2, hidden_size=2)\n",
    "output, prediction = perceptron.predict(input_data)\n",
    "error = perceptron._mean_squared_error(output, actual_output)\n",
    "print(f\"Input: {input_data}, Predição: {output}, Retorno: {prediction}, Erro: {error}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementando o processo de treinamento\n",
    "\n",
    "Em conclusão com o processo de treinamento,e ajuste dos erros, o Perceptron começou a de fato cumprir seu papel. Abaixo, é possível identificar que os valores aproximados condizem com a porta XOR, e apesar de nenhuma predição chegar perto de 1, é possível inferir que se for maior que 0.9, provavelmente a saída correta é 1. \n",
    "Por esse motivo, o print de saída mostra o input, o valor que foi estimado pelo Perceptron e também o retorno final que ele daria (definindo entre 0 e 1).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0 0], Predict: [0.0639244], Return: 0\n",
      "Input: [0 1], Predict: [0.94039637], Return: 1\n",
      "Input: [1 0], Predict: [0.94031645], Return: 1\n",
      "Input: [1 1], Predict: [0.0646812], Return: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, input_size, hidden_size, learning_rate=0.1, bias=1):\n",
    "        self.weights_input_hidden = np.random.rand(input_size, hidden_size)\n",
    "        self.weights_hidden_output = np.random.rand(hidden_size, 1)\n",
    "        self.bias_hidden = np.random.rand(hidden_size)\n",
    "        self.bias_output = np.random.rand(1)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def _sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def _mean_squared_error(self, predicted, actual):\n",
    "        return np.mean((predicted - actual) ** 2)\n",
    "\n",
    "    def forward_pass(self, data):\n",
    "        # Passagem pela camada escondida\n",
    "        self.hidden_input = np.dot(data, self.weights_input_hidden) + self.bias_hidden\n",
    "        self.hidden_output = self._sigmoid(self.hidden_input)\n",
    "        \n",
    "        # Passagem pela camada de saída\n",
    "        self.final_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output\n",
    "        self.final_output = self._sigmoid(self.final_input)\n",
    "        \n",
    "        return self.final_output\n",
    "\n",
    "    def predict(self, data):\n",
    "        output = self.forward_pass(data)\n",
    "        return output, 1 if output > 0.9 else 0\n",
    "\n",
    "    def train(self, training_inputs, training_outputs, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            for inputs, actual_output in zip(training_inputs, training_outputs):\n",
    "                # Forward pass\n",
    "                output = self.forward_pass(inputs)\n",
    "                \n",
    "                # Calcular erro na camada de saída\n",
    "                output_error = actual_output - output\n",
    "                output_delta = output_error * self._sigmoid_derivative(output)\n",
    "                \n",
    "                # Calcular erro na camada escondida\n",
    "                hidden_error = output_delta.dot(self.weights_hidden_output.T)\n",
    "                hidden_delta = hidden_error * self._sigmoid_derivative(self.hidden_output)\n",
    "                \n",
    "                # Atualizar pesos para camada escondida para camada de saída\n",
    "                self.weights_hidden_output += self.hidden_output.reshape(-1, 1).dot(output_delta.reshape(1, -1)) * self.learning_rate\n",
    "                self.bias_output += output_delta * self.learning_rate\n",
    "                \n",
    "                # Atualizar pesos para camada de entrada para camada escondida\n",
    "                self.weights_input_hidden += inputs.reshape(-1, 1).dot(hidden_delta.reshape(1, -1)) * self.learning_rate\n",
    "                self.bias_hidden += hidden_delta * self.learning_rate\n",
    "\n",
    "# Dados de treinamento XOR\n",
    "training_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "training_outputs = np.array([0, 1, 1, 0])\n",
    "\n",
    "# Treinando o Perceptron\n",
    "perceptron = Perceptron(input_size=2, hidden_size=2)\n",
    "perceptron.train(training_inputs, training_outputs, epochs=10000)\n",
    "\n",
    "# Testando o Perceptron\n",
    "for inputs in training_inputs:\n",
    "    output, prediction = perceptron.predict(inputs)\n",
    "    print(f\"Input: {inputs}, Predict: {output}, Return: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "Na versão acima, foi utilizado um ajuste simples de pesos, mas como a ponderada pede explicitamente um *backpropagation*, assim eu o farei.\n",
    "\n",
    "O Backpropagation envolve o foward pass (que é calcular uma saída dada uma entrada, o que já está sendo utilizado), o backward pass (que calcula os gradientes do erro) e o ajuste dos pesos.\n",
    "\n",
    "Abaixo, é possível visualizar nosso querido Perceptron com essa pequena atualização:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0 0], Predict: [0.06325786], Return: 0\n",
      "Input: [0 1], Predict: [0.94023273], Return: 1\n",
      "Input: [1 0], Predict: [0.94014328], Return: 1\n",
      "Input: [1 1], Predict: [0.0653762], Return: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, input_size, hidden_size, learning_rate=0.1):\n",
    "        self.weights_input_hidden = np.random.rand(input_size, hidden_size)\n",
    "        self.weights_hidden_output = np.random.rand(hidden_size, 1)\n",
    "        self.bias_hidden = np.random.rand(hidden_size)\n",
    "        self.bias_output = np.random.rand(1)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def _sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def _mean_squared_error(self, predicted, actual):\n",
    "        return np.mean((predicted - actual) ** 2)\n",
    "\n",
    "    def forward_pass(self, data):\n",
    "        # Passagem pela camada escondida\n",
    "        self.hidden_input = np.dot(data, self.weights_input_hidden) + self.bias_hidden\n",
    "        self.hidden_output = self._sigmoid(self.hidden_input)\n",
    "        \n",
    "        # Passagem pela camada de saída\n",
    "        self.final_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output\n",
    "        self.final_output = self._sigmoid(self.final_input)\n",
    "        \n",
    "        return self.final_output\n",
    "\n",
    "    def predict(self, data):\n",
    "        output = self.forward_pass(data)\n",
    "        return output, 1 if output > 0.9 else 0\n",
    "\n",
    "    def train(self, training_inputs, training_outputs, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            for inputs, actual_output in zip(training_inputs, training_outputs):\n",
    "                # Forward pass\n",
    "                output = self.forward_pass(inputs)\n",
    "                \n",
    "                # Backward pass\n",
    "                # Calcular erro na camada de saída\n",
    "                output_error = actual_output - output\n",
    "                output_delta = output_error * self._sigmoid_derivative(output)\n",
    "                \n",
    "                # Calcular erro na camada escondida\n",
    "                hidden_error = output_delta.dot(self.weights_hidden_output.T)\n",
    "                hidden_delta = hidden_error * self._sigmoid_derivative(self.hidden_output)\n",
    "                \n",
    "                # Atualizar pesos para camada escondida para camada de saída\n",
    "                self.weights_hidden_output += self.hidden_output.reshape(-1, 1).dot(output_delta.reshape(1, -1)) * self.learning_rate\n",
    "                self.bias_output += output_delta * self.learning_rate\n",
    "                \n",
    "                # Atualizar pesos para camada de entrada para camada escondida\n",
    "                self.weights_input_hidden += inputs.reshape(-1, 1).dot(hidden_delta.reshape(1, -1)) * self.learning_rate\n",
    "                self.bias_hidden += hidden_delta * self.learning_rate\n",
    "\n",
    "# Dados de treinamento XOR\n",
    "training_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "training_outputs = np.array([0, 1, 1, 0])\n",
    "\n",
    "# Treinando o Perceptron\n",
    "perceptron = Perceptron(input_size=2, hidden_size=2)\n",
    "perceptron.train(training_inputs, training_outputs, epochs=10000)\n",
    "\n",
    "# Testando o Perceptron\n",
    "for inputs in training_inputs:\n",
    "    output, prediction = perceptron.predict(inputs)\n",
    "    print(f\"Input: {inputs}, Predict: {output}, Return: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando o Pytorch\n",
    "\n",
    "Aqui eu utilizei o pytorch para fazer o perceptron, que é uma biblioteca que facilita o processo.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
